{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### how to build and use a recurrent neural network in Keras to write patent abstracts.\n",
    "\n",
    "we input a sequence of words and train the model to predict the very next word. \n",
    "\n",
    "When we go to write a new patent, we pass in a starting sequence of words, make a prediction for the next word, update the input sequence, make another prediction, add the word to the sequence and continue for however many words we want to generate.\n",
    "\n",
    "The steps of the approach are outlined below:\n",
    "1. Convert abstracts from list of strings into list of lists of integers (sequences)\n",
    "2. Create feature and labels from sequences\n",
    "3. Build LSTM model with Embedding, LSTM, and Dense layers\n",
    "4. Load in pre-trained embeddings\n",
    "5. Train model to predict next work in sequence\n",
    "6. Make predictions by passing in starting sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import HTML\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from itertools import chain\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "\n",
    "RANDOM_STATE = 50\n",
    "TRAIN_FRACTION = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"Retrieve a Keras model and embeddings\"\"\"\n",
    "    model = load_model(f'{model_name}.h5')\n",
    "    embeddings = model.get_layer(index = 0)\n",
    "    embeddings = embeddings.get_weights()[0]\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
    "    embeddings = np.nan_to_num(embeddings)\n",
    "    word_idx = []\n",
    "    with open(f'training-rnn.json', 'rb') as f:\n",
    "        for l in f:\n",
    "            word_idx.append(json.loads(l))\n",
    "        \n",
    "    word_idx = word_idx[0]\n",
    "    word_idx['UNK'] = 0\n",
    "    idx_word = {index: word for word, index in word_idx.items()}\n",
    "    return model, embeddings, word_idx, idx_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model):\n",
    "    \"\"\"Retrieve the embeddings in a model\"\"\"\n",
    "    embeddings = model.get_layer(index = 0)\n",
    "    embeddings = embeddings.get_weights()[0]\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
    "    embeddings = np.nan_to_num(embeddings)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(query, embedding_matrix, word_idx, idx_word, n = 10):\n",
    "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
    "    \n",
    "    idx = word_idx.get(query, None)\n",
    "    # Handle case where query is not in vocab\n",
    "    if idx is None:\n",
    "        print(f'{query} not found in vocab.')\n",
    "        return\n",
    "    else:\n",
    "        vec = embedding_matrix[idx]\n",
    "        # Handle case where word doesn't have an embedding\n",
    "        if np.all(vec == 0):\n",
    "            print(f'{query} has no pre-trained embedding.')\n",
    "            return\n",
    "        else:\n",
    "            # Calculate distance between vector and all others\n",
    "            dists = np.dot(embedding_matrix, vec)\n",
    "            \n",
    "            # Sort indexes in reverse order\n",
    "            idxs = np.argsort(dists)[::-1][:n]\n",
    "            sorted_dists = dists[idxs]\n",
    "            closest = [idx_word[i] for i in idxs]\n",
    "            \n",
    "    print(f'Query: {query}\\n')\n",
    "    # Print out the word and cosine distances\n",
    "    for word, dist in zip(closest, sorted_dists):\n",
    "        print(f'Word: {word:15} Cosine Similarity: {round(dist, 4)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sequence(s):\n",
    "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
    "    \n",
    "    # Add spaces around punctuation\n",
    "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
    "    \n",
    "    # Remove references to figures\n",
    "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
    "    \n",
    "    # Remove double spaces\n",
    "    s = re.sub(r'\\s\\s', ' ', s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(s):\n",
    "    \"\"\"Remove spaces around punctuation\"\"\"\n",
    "    s = re.sub(r'\\s+([.,;?])', r'\\1', s)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, filters='!\"%;[\\\\]^_`{|}~\\t\\n', training_len=50,\n",
    "             lower=False):\n",
    "    \"\"\"Retrieve formatted training and validation data from a file\"\"\"\n",
    "    \n",
    "    data = pd.read_csv(file, parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])\n",
    "    abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]\n",
    "    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n",
    "        abstracts, training_len, lower, filters)\n",
    "    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
    "    training_dict = {'X_train': X_train, 'X_valid': X_valid, \n",
    "                     'y_train': y_train, 'y_valid': y_valid}\n",
    "    return training_dict, word_idx, idx_word, sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid(features,\n",
    "                       labels,\n",
    "                       num_words,\n",
    "                       train_fraction=0.7):\n",
    "    \"\"\"Create training and validation features and labels.\"\"\"\n",
    "    \n",
    "    # Randomly shuffle features and labels\n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Decide on number of samples for training\n",
    "    train_end = int(train_fraction * len(labels))\n",
    "\n",
    "    train_features = np.array(features[:train_end])\n",
    "    valid_features = np.array(features[train_end:])\n",
    "\n",
    "    train_labels = labels[:train_end]\n",
    "    valid_labels = labels[train_end:]\n",
    "\n",
    "    # Convert to arrays\n",
    "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "    # Using int8 for memory savings\n",
    "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(train_labels):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(valid_labels):\n",
    "        y_valid[example_index, word_index] = 1\n",
    "\n",
    "    # Memory management\n",
    "    import gc\n",
    "    gc.enable()\n",
    "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts, training_length = 50,\n",
    "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "    \n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n",
    "    \n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "    \n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "        \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "        \n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length: i + 1]\n",
    "            \n",
    "            # Set the features and label\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    print(f'There are {len(features)} sequences.')\n",
    "    \n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model,\n",
    "                    sequences,\n",
    "                    idx_word,\n",
    "                    seed_length=50,\n",
    "                    new_words=50,\n",
    "                    diversity=1,\n",
    "                    return_output=False,\n",
    "                    n_gen=1):\n",
    "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
    "\n",
    "    # Choose a random sequence\n",
    "    seq = random.choice(sequences)\n",
    "\n",
    "    # Choose a random starting point\n",
    "    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
    "    # Ending index for seed\n",
    "    end_idx = seed_idx + seed_length\n",
    "\n",
    "    gen_list = []\n",
    "\n",
    "    for n in range(n_gen):\n",
    "        # Extract the seed sequence\n",
    "        seed = seq[seed_idx:end_idx]\n",
    "        original_sequence = [idx_word[i] for i in seed]\n",
    "        generated = seed[:] + ['#']\n",
    "\n",
    "        # Find the actual entire sequence\n",
    "        actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
    "\n",
    "        # Keep adding new words\n",
    "        for i in range(new_words):\n",
    "\n",
    "            # Make a prediction from the seed\n",
    "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
    "                np.float64)\n",
    "\n",
    "            # Diversify\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "\n",
    "            # Softmax\n",
    "            preds = exp_preds / sum(exp_preds)\n",
    "\n",
    "            # Choose the next word\n",
    "            probas = np.random.multinomial(1, preds, 1)[0]\n",
    "\n",
    "            next_idx = np.argmax(probas)\n",
    "\n",
    "            # New seed adds on old word\n",
    "            #             seed = seed[1:] + [next_idx]\n",
    "            seed += [next_idx]\n",
    "            generated.append(next_idx)\n",
    "\n",
    "        # Showing generated and actual abstract\n",
    "        n = []\n",
    "\n",
    "        for i in generated:\n",
    "            n.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "        gen_list.append(n)\n",
    "\n",
    "    a = []\n",
    "\n",
    "    for i in actual:\n",
    "        a.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "    a = a[seed_length:]\n",
    "\n",
    "    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
    "\n",
    "    if return_output:\n",
    "        return original_sequence, gen_list, a\n",
    "\n",
    "    # HTML formatting\n",
    "    seed_html = ''\n",
    "    seed_html = addContent(seed_html, header(\n",
    "        'Seed Sequence', color='darkblue'))\n",
    "    seed_html = addContent(seed_html,\n",
    "                           box(remove_spaces(' '.join(original_sequence))))\n",
    "\n",
    "    gen_html = ''\n",
    "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
    "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n",
    "\n",
    "    a_html = ''\n",
    "    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n",
    "    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n",
    "\n",
    "    return seed_html, gen_html, a_html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header(text, color = 'black', gen_text = None):\n",
    "    if gen_text:\n",
    "        raw_html = f'<h1 style=\"color: {color};\"><p><center>' + str(\n",
    "        text) + '<span style=\"color: red\">' + str(gen_text) + '</center></p></h1>'\n",
    "    else:\n",
    "        raw_html = f'<h1 style=\"color: {color};\"><center>' + str(\n",
    "            text) + '</center></h1>'\n",
    "    return raw_html\n",
    "\n",
    "\n",
    "def box(text, gen_text=None):\n",
    "    if gen_text:\n",
    "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>' + str(\n",
    "            text) +'<span style=\"color: red\">' + str(gen_text) + '</p></div>'\n",
    "\n",
    "    else:\n",
    "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + str(\n",
    "            text) + '</div>'\n",
    "    return raw_html\n",
    "\n",
    "\n",
    "def addContent(old_html, raw_html):\n",
    "    old_html += raw_html\n",
    "    return old_html\n",
    "\n",
    "def seed_sequence(model, s, word_idx, idx_word, \n",
    "                  diversity = 0.75, num_words = 50):\n",
    "    \"\"\"Generate output starting from a seed sequence.\"\"\"\n",
    "    # Original formated text\n",
    "    start = format_sequence(s).split()\n",
    "    gen = []\n",
    "    s = start[:]\n",
    "    # Generate output\n",
    "    for _ in range(num_words):\n",
    "        # Conver to arry\n",
    "        x = np.array([word_idx.get(word, 0) for word in s]).reshape((1, -1))\n",
    "\n",
    "        # Make predictions\n",
    "        preds = model.predict(x)[0].astype(float)\n",
    "\n",
    "        # Diversify\n",
    "        preds = np.log(preds) / diversity\n",
    "        exp_preds = np.exp(preds)\n",
    "        # Softmax\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        # Pick next index\n",
    "        next_idx = np.argmax(np.random.multinomial(1, preds, size = 1))\n",
    "        s.append(idx_word[next_idx])\n",
    "        gen.append(idx_word[next_idx])\n",
    "    \n",
    "    # Formatting in html\n",
    "    start = remove_spaces(' '.join(start)) + ' '\n",
    "    gen = remove_spaces(' '.join(gen)) \n",
    "    html = ''\n",
    "    html = addContent(html, header('Input Seed ', color = 'black', gen_text = 'Network Output'))\n",
    "    html = addContent(html, box(start, gen))\n",
    "    return html\n",
    "\n",
    "def guess_human(model, sequences, idx_word, seed_length=50):\n",
    "    \"\"\"Produce 2 RNN sequences and play game to compare to actaul.\n",
    "       Diversity is randomly set between 0.5 and 1.25\"\"\"\n",
    "    \n",
    "    new_words = np.random.randint(10, 50)\n",
    "    diversity = np.random.uniform(0.5, 1.25)\n",
    "    sequence, gen_list, actual = generate_output(model, sequences, idx_word, seed_length, new_words,\n",
    "                                                 diversity=diversity, return_output=True, n_gen = 2)\n",
    "    gen_0, gen_1 = gen_list\n",
    "    \n",
    "    output = {'sequence': remove_spaces(' '.join(sequence)),\n",
    "              'computer0': remove_spaces(' '.join(gen_0)),\n",
    "              'computer1': remove_spaces(' '.join(gen_1)),\n",
    "              'human': remove_spaces(' '.join(actual))}\n",
    "    \n",
    "    print(f\"Seed Sequence: {output['sequence']}\\n\")\n",
    "    \n",
    "    choices = ['human', 'computer0', 'computer1']\n",
    "          \n",
    "    selected = []\n",
    "    i = 0\n",
    "    while len(selected) < 3:\n",
    "        choice = random.choice(choices)\n",
    "        selected.append(choice)\n",
    "        print(f'\\nOption {i + 1} {output[choice]}')\n",
    "        choices.remove(selected[-1])\n",
    "        i += 1\n",
    "    \n",
    "    print('\\n')\n",
    "    guess = int(input('Enter option you think is human (1-3): ')) - 1\n",
    "    print('\\n')\n",
    "    \n",
    "    if guess == np.where(np.array(selected) == 'human')[0][0]:\n",
    "        print('*' * 3 + 'Correct' + '*' * 3 + '\\n')\n",
    "        print('-' * 60)\n",
    "        print('Ordering: ', selected)\n",
    "    else:\n",
    "        print('*' * 3 + 'Incorrect' + '*' * 3 + '\\n')\n",
    "        print('-' * 60)\n",
    "        print('Correct Ordering: ', selected)\n",
    "          \n",
    "    print('Diversity', round(diversity, 2))\n",
    "    \n",
    "def make_sequences_new(texts,\n",
    "                   training_length=50,\n",
    "                   lower=True,\n",
    "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Limit to sequences with more than (training length + 20) tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [\n",
    "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
    "    ]\n",
    "\n",
    "    new_texts = []\n",
    "\n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "    \n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    # Refit on long texts\n",
    "    tokenizer.fit_on_texts(new_texts)\n",
    "    new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "    \n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f'There are {num_words} unique words.')\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "\n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length:i + 1]\n",
    "\n",
    "            # Set the features and label\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "\n",
    "    print(f'There are {len(features)} training sequences.')\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Training Data\n",
    "\n",
    "The raw data for this project comes from USPTO PatentsView, where you can search for information on any patent applied for in the United States.\n",
    "\n",
    "https://www.patentsview.org/querydev/\n",
    "\n",
    "* Using patent abstracts from patent search for neural network\n",
    "* 3000+ patents total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
       "      <td>1996-07-09</td>\n",
       "      <td>5535303</td>\n",
       "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" This invention is a novel high-speed neural ...</td>\n",
       "      <td>1993-10-19</td>\n",
       "      <td>5255349</td>\n",
       "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An optical information processor for use as a ...</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>5383042</td>\n",
       "      <td>3 layer liquid crystal neural network with out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>6169981</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2003-06-17</td>\n",
       "      <td>6581048</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     patent_abstract patent_date  \\\n",
       "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
       "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
       "2  An optical information processor for use as a ...  1995-01-17   \n",
       "3  A method and system for intelligent control of...  2001-01-02   \n",
       "4  A method and system for intelligent control of...  2003-06-17   \n",
       "\n",
       "  patent_number                                       patent_title  \n",
       "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
       "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
       "2       5383042  3 layer liquid crystal neural network with out...  \n",
       "3       6169981  3-brain architecture for an intelligent decisi...  \n",
       "4       6581048  3-brain architecture for an intelligent decisi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('neural_network_patent_query.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16192 unique words.\n",
      "There are 318563 sequences.\n"
     ]
    }
   ],
   "source": [
    "training_dict, word_idx, idx_word, sequences = get_data('neural_network_patent_query.txt', training_len = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3255"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequences of text are represented as integers\n",
    "    * `word_idx` maps words to integers\n",
    "    * `idx_word` maps integers to words\n",
    "* Features are integer sequences of length 50\n",
    "* Label is next word in sequence\n",
    "* Labels are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  117,     7,   141,   277,     4,    18,    81,   110,    10,\n",
       "          219,    29,     1,   952,  2453,    19,     5,     6,     1,\n",
       "          117,    10,   182,  2166,    21,     1,    81,   178,     4,\n",
       "           13,   117,   894,    14,  6163,     7,   302,     1,     9,\n",
       "            8,    29,    33,    23,    74,   428,     7,   692,     1,\n",
       "           81,   183,     4,    13,   117],\n",
       "       [    6,    41,     2,    87,     3,  1340,    79,     7,     1,\n",
       "          409,   543,    22,   484,     6,     2,  2113,   728,    24,\n",
       "            1,   178,     3,     1,  1820,    55,    14, 13942,  7240,\n",
       "          244,     5,    14, 13943,  7240,   244,     5,     2,  2113,\n",
       "         7240,   244,     5,     2,    38,  9292,   244,     2,    49,\n",
       "         9292,   244,    14,    22, 13944]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dict['X_train'][:2]\n",
    "training_dict['y_train'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous step converts all the abstracts to sequences of integers. The next step is to create a supervised machine learning problem with which to train the network.\n",
    "\n",
    "Give the network a sequence of words and train it to predict the next word.\n",
    "\n",
    "The number of words is left as a parameter; we’ll use 50 for the examples shown here which means we give our network 50 words and train it to predict the 51st.\n",
    "\n",
    "We use the first 50 words as features with the 51st as the label, then use words 2–51 as features and predict the 52nd and so on. This gives us significantly more training data which is beneficial because the performance of the network is proportional to the amount of data that it sees during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: user to provide samples . A recognition operation is performed on the user's handwritten input , and the user is not satisfied with the recognition result . The user selects an option to train the neural network on one or more characters to improve the recognition results . The user\n",
      "\n",
      "Label: is\n",
      "\n",
      "Features: and includes a number of amplifiers corresponding to the N bit output sum and a carry generation from the result of the adding process an augend input-synapse group , an addend input-synapse group , a carry input-synapse group , a first bias-synapse group a second bias-synapse group an output feedback-synapse\n",
      "\n",
      "Label: group\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
    "    text = []\n",
    "    for idx in sequence:\n",
    "        text.append(idx_word[idx])\n",
    "        \n",
    "    print('Features: ' + ' '.join(text) + '\\n')\n",
    "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222994, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dict['X_train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features end up with shape (296866, 50) which means we have almost 300,000 sequences each with 50 tokens. In the language of recurrent neural networks, each sequence has 50 timesteps each with 1 feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Recurrent Neural Network\n",
    "\n",
    "* Embedding dimension = 100\n",
    "* 64 LSTM cells in one layer\n",
    "    * Dropout and recurrent dropout for regularization\n",
    "* Fully connected layer with 64 units on top of LSTM\n",
    "     * 'relu' activation\n",
    "* Drop out for regularization\n",
    "* Output layer produces prediction for each word\n",
    "    * 'softmax' activation\n",
    "* Adam optimizer with defaults\n",
    "* Categorical cross entropy loss\n",
    "* Monitor accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Keras Sequential API which means we build the network up one layer at a time. The layers are as follows:\n",
    "1. An Embedding which maps each input word to a 100-dimensional vector. The embedding can use pre-trained weights which we supply in the weights parameter. trainable can be set False if we don’t want to update the embeddings.\n",
    "2. A Masking layer to mask any words that do not have a pre-trained embedding which will be represented as all zeros. This layer should not be used when training the embeddings.\n",
    "3. The heart of the network: a layer of LSTM cells with dropout to prevent overfitting. Since we are only using one LSTM layer, it does not return the sequences, for using two or more layers, make sure to return sequences.\n",
    "4. A fully-connected Dense layer with relu activation. This adds additional representational capacity to the network.\n",
    "5. A Dropout layer to prevent overfitting to the training data.\n",
    "6. A Dense fully-connected output layer. This produces a probability for every word in the vocab using softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_idx) + 1,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(word_idx) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Pre-Trained Model\n",
    "\n",
    "Rather than waiting several hours to train the model, we can load in a model trained for 150 epochs. We'll demonstrate how to train this model for another 5 epochs which shouldn't take too long depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 222994 samples, validate on 95569 samples\n",
      "Epoch 1/5\n",
      "222994/222994 [==============================] - 50s 223us/step - loss: 3.7555 - acc: 0.2943 - val_loss: 4.7447 - val_acc: 0.2672\n",
      "Epoch 2/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7455 - acc: 0.2961 - val_loss: 4.7355 - val_acc: 0.2679\n",
      "Epoch 3/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7317 - acc: 0.2963 - val_loss: 4.7395 - val_acc: 0.2691\n",
      "Epoch 4/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7209 - acc: 0.2966 - val_loss: 4.7410 - val_acc: 0.2692\n",
      "Epoch 5/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7091 - acc: 0.2979 - val_loss: 4.7367 - val_acc: 0.2707\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load in model and demonstrate training\n",
    "model = load_model('train-embeddings-rnn.h5')\n",
    "h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs = 5, batch_size = 2048, \n",
    "          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: Log Loss and Accuracy on training data\n",
      "222994/222994 [==============================] - 24s 109us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.282083551313851, 0.33844408377189383]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance: Log Loss and Accuracy on validation data\n",
      "95569/95569 [==============================] - 10s 108us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.737925765920241, 0.2671891513580688]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('../models/train-embeddings-rnn.h5')\n",
    "print('Model Performance: Log Loss and Accuracy on training data')\n",
    "model.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n",
    "\n",
    "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
    "model.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a minor amount of overfitting on the training data but it's not major. Using regularization in both the LSTM layer and after the fully dense layer can help to combat the prevalent issue of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Output\n",
    "\n",
    "We can use the fully trained model to generate output by starting it off with a seed sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">obtaining empirical data of parameters for an existing integrated circuit manufacturing process and extrapolating the known data to a new technology to assess potential yields of the new technology from the known process. Further, process variables of the new process may be adjusted based upon the empirical data</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > based on the object. Therefore, the invention is a simple controller. In addition, a computational system is provided with other independent data. The method also</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > in order to optimize the yields of the new technology. A logic based computing system such as a fuzzy logic or neural-network system may be utilized. The computing</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in generate_output(model, sequences, idx_word, seed_length = 50, new_words = 30, diversity = 0.75):\n",
    "    HTML(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">circuit configuration and compact circuit size. With the presently cutting-edge technology (0.15 μm CMOS), approximately 1G synapses can be integrated on one chip. Accordingly, it is</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > trained automatically close an inner cell universal size thus reduces component weights as signals by known training technique to parameters close exposures and predefined instructions the data activation. The</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > possible to implement a neural network with approximately 30,000 neurons all coupled together on one chip. This corresponds to a network scale capable of associatively storing approximately 5,000 patterns</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in generate_output(model, sequences, idx_word, seed_length = 30, new_words = 30, diversity = 1.5):\n",
    "    HTML(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too high of a diversity and the output will be nearly random. Too low of a diversity and the model can get stuck outputting loops of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network with own input\n",
    "\n",
    "Here you can input your own starting sequence for the network. The network will produce `num_words` of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>This patent provides a basis for using a recurrent neural network to <span style=\"color: red\">simple and automobiles, or or type or key computer to guide the original image. The neural network can</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'This patent provides a basis for using a recurrent neural network to '\n",
    "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>The cell state is passed along from one time step to another allowing the <span style=\"color: red\">group of data to emulate a pre-determined model neural network. The neural network includes a set of nodes to</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The cell state is passed along from one time step to another allowing the '\n",
    "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Embeddings\n",
    "\n",
    "As a final piece of model inspection, we can look at the embeddings and find the words closest to a query word in the embedding space. This gives us an idea of what the network has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16192, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embeddings(model)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the vocabulary is now represented as a 100-dimensional vector. This could be reduced to 2 or 3 dimensions for visualization. It can also be used to find the closest word to a query word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: network\n",
      "\n",
      "Word: network         Cosine Similarity: 1.0\n",
      "Word: channel         Cosine Similarity: 0.7754999995231628\n",
      "Word: networks        Cosine Similarity: 0.7745000123977661\n",
      "Word: system          Cosine Similarity: 0.7559999823570251\n",
      "Word: program         Cosine Similarity: 0.7541999816894531\n",
      "Word: cable           Cosine Similarity: 0.7419999837875366\n",
      "Word: now             Cosine Similarity: 0.7297999858856201\n",
      "Word: programming     Cosine Similarity: 0.7179999947547913\n",
      "Word: web             Cosine Similarity: 0.7138000130653381\n",
      "Word: line            Cosine Similarity: 0.6915000081062317\n"
     ]
    }
   ],
   "source": [
    "find_closest('network', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word should have a cosine similarity of 1.0 with itself! The embeddings are learned for a task, so the nearest words may only make sense in the context of the patents on which we trained the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: data\n",
      "\n",
      "Word: data            Cosine Similarity: 1.0\n",
      "Word: information     Cosine Similarity: 0.8185999989509583\n",
      "Word: numbers         Cosine Similarity: 0.683899998664856\n",
      "Word: database        Cosine Similarity: 0.6776000261306763\n",
      "Word: account         Cosine Similarity: 0.6575999855995178\n",
      "Word: report          Cosine Similarity: 0.6575999855995178\n",
      "Word: signals         Cosine Similarity: 0.6399999856948853\n",
      "Word: system          Cosine Similarity: 0.6377000212669373\n",
      "Word: statistics      Cosine Similarity: 0.6371999979019165\n",
      "Word: web             Cosine Similarity: 0.6359000205993652\n"
     ]
    }
   ],
   "source": [
    "find_closest('data', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the network has learned some basic relationships between words! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
